# Contact Center Sentiment Analysis (Async LLM Pipeline)

A production-style, async sentiment analysis pipeline for contact-center conversations using **Python**, **Ollama (local LLMs)**, and **batched concurrency**.  
Designed to demonstrate real-world data engineering and applied AI patterns such as async execution, throughput optimization, and operational metrics.

---

## üîç Project Overview

This project analyzes customer sentiment from contact-center conversations by:
- Extracting **customer-only messages**
- Running **local LLM inference** via Ollama
- Processing data using **async + batch + parallel execution**
- Capturing **latency, throughput, and system metrics**
- Producing **analytics-ready outputs (CSV + JSON)**

The architecture mirrors how large-scale NLP pipelines are built in production systems.

---

##  Key Features

- **Async + Parallel Processing**
  - Uses `asyncio` + `aiohttp`
  - Configurable concurrency limits to protect system resources

- **Batch-Oriented Execution**
  - Conversations processed in batches for predictable throughput
  - Per-batch timing and performance tracking

- **Local LLM Inference (Ollama)**
  - No external API dependency
  - Supports models like `llama3.2`, `phi3`, `mistral`

- **Resilient JSON Parsing**
  - Handles imperfect LLM outputs
  - Gracefully falls back to raw model output when parsing fails

- **Operational Metrics**
  - Per-conversation latency
  - Batch throughput
  - CPU & memory snapshots

- **Production-Style Code Structure**
  - Engine-driven execution
  - Clear separation of responsibilities across modules

---


## üìÇ Project Structure

contact-center-sentiment
- engine.py ‚Äî Main entry point (run this file)
- config.py ‚Äî Central configuration
- data_processing.py ‚Äî CSV loading & conversation structuring
- ollama_client.py ‚Äî Async Ollama inference client
- batch_runner.py ‚Äî Batch + concurrency orchestration
- metrics.py ‚Äî Output writing & run statistics
- requirements.txt ‚Äî Python dependencies
- synthetic_conversations_1M_input.csv
- README.md


---

## ‚öôÔ∏è Setup Instructions (Windows / VS Code)

### 1Ô∏è‚É£ Prerequisites
- Python **3.10+**
- Ollama installed and running  
  ```bash
  ollama run llama3.2

2Ô∏è‚É£ Clone the Repository
git clone https://github.com/Sadanand-AI-Engineer/contact-center-sentiment.git
cd contact-center-sentiment

3Ô∏è‚É£ Create & Activate Virtual Environment
python -m venv .venv
.venv\Scripts\Activate.ps1

4Ô∏è‚É£ Run the Pipeline
python engine.py


---The engine automatically installs missing dependencies from requirements.txt on first run.

üì§ Output Files Explained

After execution, the following files are generated:

1Ô∏è‚É£ sentiment_results_ollama_optimized.csv

Clean, analytics-ready output

conversation_id

sentiment (positive / neutral / negative)

score (0‚Äì1 confidence)

summary (short explanation)

üëâ Use this for dashboards, BI tools, or downstream analytics.

2Ô∏è‚É£ sentiment_results_ollama_optimized_full.csv

Full diagnostic output
Includes:

Raw LLM responses

Retry attempts

Per-conversation latency

Error flags (json_parse_failed, network errors)

üëâ Useful for debugging and model behavior analysis.

3Ô∏è‚É£ batch_stats_ollama_optimized.csv

Batch-level performance metrics

Batch duration

Throughput (conversations/sec)

üëâ Shows how performance scales with concurrency.

4Ô∏è‚É£ run_stats_ollama_optimized.json

Run-level metadata

Total runtime

Overall throughput

CPU & memory usage

System information

Useful for capacity planning and benchmarking.

**Design Decisions (Why This Matters)**

Customer-only text ‚Üí focuses sentiment on customer experience

Character & message limits ‚Üí reduces tokens, improves speed

Async + batching ‚Üí mimics real production inference pipelines

Local LLMs ‚Üí cost-free, privacy-friendly experimentation

üë§ Author

Sadanand
AI / Data Engineering | Async pipelines | LLM systems | Python
